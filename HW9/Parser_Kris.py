# -*- coding: utf-8 -*-
"""Parser.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FmabqvhPvFlYShZyCauHyEcuoBUrhxVm
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

# Forward LSTM: Processes sequences from start to end
class ForwardLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, dropout=0.3):
        super(ForwardLSTM, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)  # LSTM for forward direction
        self.dropout = nn.Dropout(dropout)  # Dropout for regularization

    def forward(self, x):
        _, (h_n, _) = self.lstm(x)  # Get the final hidden state
        return self.dropout(h_n[-1])  # Apply dropout and return final hidden state

# Backward LSTM: Processes sequences from end to start
class BackwardLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, dropout=0.3):
        super(BackwardLSTM, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)  # LSTM for backward direction
        self.dropout = nn.Dropout(dropout)  # Dropout for regularization

    def forward(self, x):
        _, (h_n, _) = self.lstm(x)  # Get the final hidden state
        return self.dropout(h_n[-1])  # Apply dropout and return final hidden state

# Bidirectional LSTM: Combines forward and backward representations
class BiLSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, dropout=0.3):
        super(BiLSTM, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)  # Bi-directional LSTM
        self.dropout = nn.Dropout(dropout)  # Dropout for regularization

    def forward(self, x):
        output, _ = self.lstm(x)  # Get the output from both directions
        return self.dropout(output)  # Apply dropout and return the combined output

# Main parser model
class Parser(nn.Module):
    def __init__(
        self, input_dim, char_hidden_dim, word_hidden_dim, span_hidden_dim, output_dim, dropout=0.3, vocab_size=26
    ):
        super(Parser, self).__init__()
        self.embedding = nn.Embedding(vocab_size, input_dim)  # Embedding layer for character IDs
        self.forward_lstm = ForwardLSTM(input_dim, char_hidden_dim, dropout)  # Forward LSTM for suffixes
        self.backward_lstm = BackwardLSTM(input_dim, char_hidden_dim, dropout)  # Backward LSTM for prefixes
        self.bilstm = BiLSTM(char_hidden_dim * 2, word_hidden_dim, dropout)  # BiLSTM for word representations
        self.ffn = nn.Sequential(  # Feedforward network for span classification
            nn.Linear(word_hidden_dim * 2, span_hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(span_hidden_dim, output_dim),
        )

    def forward(self, suffixes, prefixes):
        # Step 1: Embedding the suffixes and prefixes
        suffixes = self.embedding(suffixes.long())  # Embed suffix IDs
        prefixes = self.embedding(prefixes.long())  # Embed prefix IDs

        # Reshape to 3D for LSTM processing
        batch_size, sentence_length, prefix_suffix_length, input_dim = suffixes.size()
        suffixes = suffixes.view(-1, prefix_suffix_length, input_dim)  # Reshape for LSTM
        prefixes = prefixes.view(-1, prefix_suffix_length, input_dim)  # Reshape for LSTM

        # Step 2: Compute word representations
        forward_repr = self.forward_lstm(suffixes)  # Process suffixes with forward LSTM
        backward_repr = self.backward_lstm(prefixes)  # Process prefixes with backward LSTM

        # Reshape back to 3D
        forward_repr = forward_repr.view(batch_size, sentence_length, -1)
        backward_repr = backward_repr.view(batch_size, sentence_length, -1)
        word_repr = torch.cat((forward_repr, backward_repr), dim=-1)  # Concatenate forward and backward representations

        # Step 3: Compute sentence representations
        dummy_vector = torch.zeros((batch_size, 1, word_repr.size(-1)), device=word_repr.device)  # Dummy vector for padding
        bilstm_input = torch.cat((dummy_vector, word_repr, dummy_vector), dim=1)  # Add dummy vectors to both ends
        sentence_repr = self.bilstm(bilstm_input)  # Process with BiLSTM

        # Separate forward and backward components
        forward_repr = sentence_repr[:, :-1, :word_repr.size(-1)]  # Forward components
        backward_repr = sentence_repr[:, 1:, word_repr.size(-1):]  # Backward components

        # Step 4: Compute span representations
        num_positions = forward_repr.size(1)  # Number of word positions
        start_positions, end_positions = torch.triu_indices(num_positions, num_positions, offset=1)  # Upper triangle indices

        # Adjust for batch processing
        forward_start = forward_repr[:, start_positions, :]  # Start positions (forward)
        forward_end = forward_repr[:, end_positions, :]  # End positions (forward)
        backward_start = backward_repr[:, start_positions, :]  # Start positions (backward)
        backward_end = backward_repr[:, end_positions, :]  # End positions (backward)

        forward_diff = forward_end - forward_start  # Difference for forward representations
        backward_diff = backward_start - backward_end  # Difference for backward representations
        span_reprs = torch.cat((forward_diff, backward_diff), dim=-1)  # Concatenate differences

        # Step 5: Predict categories
        scores = self.ffn(span_reprs)  # Apply feedforward network
        scores[:, :, 0] = 0  # Ensure "keine Konstituente" has score 0

        return scores  # Return span classification scores

# Test script
if __name__ == "__main__":
    # Test parameters
    batch_size = 2  # Number of sentences in the batch
    sentence_length = 5  # Length of the sentences
    prefix_suffix_length = 10  # Length of the prefixes/suffixes
    char_embedding_dim = 50  # Dimension of character embeddings
    char_hidden_dim = 64  # Hidden state dimension of the word LSTMs
    word_hidden_dim = 128  # Hidden state dimension of the sentence LSTMs
    span_hidden_dim = 256  # Dimension of the hidden layer in the feedforward network
    num_categories = 10  # Number of syntactic categories (including "keine Konstituente")

    # Random input data (character IDs for prefixes/suffixes)
    suffixes = torch.randint(0, 26, (batch_size, sentence_length, prefix_suffix_length))  # Random suffix data
    prefixes = torch.randint(0, 26, (batch_size, sentence_length, prefix_suffix_length))  # Random prefix data

    # Initialize the model
    parser = Parser(
        input_dim=char_embedding_dim,
        char_hidden_dim=char_hidden_dim,
        word_hidden_dim=word_hidden_dim,
        span_hidden_dim=span_hidden_dim,
        output_dim=num_categories,
        dropout=0.3,
        vocab_size=26,
    )

    # Apply the model to the input data
    scores = parser(suffixes, prefixes)

    # Verify output
    num_spans = (sentence_length * (sentence_length + 1)) // 2  # Correct number of spans
    assert scores.size() == (batch_size, num_spans, num_categories), (
        f"Output dimensions are incorrect: {scores.size()}, "
        f"expected: ({batch_size}, {num_spans}, {num_categories})"
    )

    print("Test successful!")
    print(f"Output dimensions: {scores.size()}")

